{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14e260e4",
   "metadata": {},
   "source": [
    "## Computer Vision CS766\n",
    "Final Project\n",
    "2023\n",
    "\n",
    "Hailey Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeead442",
   "metadata": {},
   "source": [
    "### Purpose \n",
    "I am a researcher in Human-Computer Interaction (HCI) and I took this class to assist me in my research project. I am working to develop assistive technologies for adults with Down syndrome after they have left school with the aim to increase job oppurtunities. For the initial evaluation of the technologies, we need to test if a user can use the system to complete a multi-step task. Our plan is to walk our participants through a Lego building task. \n",
    "\n",
    "For this project I have attempted to construct an object detection system that informs the user if they have correctly completed the current step or if they need to try again. \n",
    "\n",
    "### Process\n",
    "First, I took images of indiviudal legos from my lego set to create the basis of my dataset. (Note: they are all in the same lighting conditions) \n",
    "\n",
    "Second, I took images of each step in the multi-step bulding process along with wrong step images for testing purposes.\n",
    "\n",
    "Third, I created a looped process that checks each update the user makes in their building process to determine if they have completed the correct step. If they have not, it rechecks that same step until they have completed it correctly. \n",
    "\n",
    "I performed image processing techniques learned in class, isolation of the new Lego that was placed in the construction, detection to determine if the new Lego is the correct peice, then detection if the peice is placed correctly on the whole construction. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758d2c98",
   "metadata": {},
   "source": [
    "### Correct step videos:\n",
    "1. base.mp4\n",
    "2. step1.mp4\n",
    "3. step2.mp4\n",
    "4. step3.mp4\n",
    "5. step4.mp4\n",
    "6. step5.mp4\n",
    "7. step6.mp4\n",
    "8. step7.mp4\n",
    "9. step8.mp4\n",
    "10. step9.mp4\n",
    "11. step10.mp4\n",
    "12. step11.mp4\n",
    "13. step12.mp4\n",
    "\n",
    "### Lego videos:\n",
    "1. lego1.mp4\n",
    "2. lego2.mp4\n",
    "3. lego3.mp4\n",
    "4. lego4.mp4\n",
    "5. lego5.mp4\n",
    "6. lego6.mp4\n",
    "7. lego7.mp4\n",
    "8. lego8.mp4\n",
    "9. lego9.mp4\n",
    "\n",
    "### Testing videos:\n",
    "1. wrong2.mp4\n",
    "2. wrong4.mp4\n",
    "3. wrong5.mp4\n",
    "4. wrong6.mp4\n",
    "5. wrong8.mp4\n",
    "6. wrongEnd.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cf5d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "videos = [\n",
    "    'step1.mp4',\n",
    "    'step2.mp4',\n",
    "    'step3.mp4'\n",
    "]\n",
    "\n",
    "imgs = [\n",
    "    'step1_0003.png',\n",
    "    'step2_0003.png',\n",
    "    'step3_0003.png'\n",
    "]\n",
    "\n",
    "colors = [\n",
    "    [[228, 101, 227], [106, 51, 132]],\n",
    "    [[255, 169, 231], [125, 38, 148]],\n",
    "    [[224,  83,  90], [112,  33,  39]]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37827169",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find upper color\n",
    "\n",
    "def click_callback(event, x, y, flags, param):\n",
    "    img, pixel_color, click_counter = param  # Unpack the parameters\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        pixel_color[0] = img[y, x]\n",
    "        print(\"Color at ({}, {}): {}\".format(x, y, pixel_color[0]))\n",
    "        click_counter[0] += 1\n",
    "\n",
    "def find_color(path):\n",
    "    img = cv2.imread(path)\n",
    "    \n",
    "    # Preprocess the img\n",
    "    #glare filtering\n",
    "    # Convert the frame from RGB to HSV color space\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    # Define the thresholds for the glare color\n",
    "    lower_glare = np.array([0, 0, 200])\n",
    "    upper_glare = np.array([180, 150, 255])\n",
    "    # Create a mask for the glare color\n",
    "    mask = cv2.inRange(hsv, lower_glare, upper_glare)\n",
    "    # Perform morphological operations to remove noise and fill in gaps\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    mask = cv2.erode(mask, kernel, iterations=2)\n",
    "    mask = cv2.dilate(mask, kernel, iterations=2)\n",
    "    # Apply the matte effect to reduce the intensity of the glare\n",
    "    img[mask > 0] -= 40\n",
    "    \n",
    "    cv2.namedWindow('image')\n",
    "    click_counter = [0]\n",
    "    pixel_color = [None]  # Define the pixel_color as a mutable list\n",
    "    cv2.setMouseCallback('image', click_callback, param=(img, pixel_color, click_counter))\n",
    "\n",
    "    while True:\n",
    "        cv2.imshow('image', img)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if click_counter[0] >= 1 or key == ord('q'):\n",
    "            break\n",
    "    print(\"Color : {}\".format(pixel_color[0]))\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    return pixel_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9540490a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for idx in range(len(videos)):\n",
    "    # Load the video and image files\n",
    "    video_path = videos[idx]\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    output_path = \"test_output_\" + video_path\n",
    "    # Get the video dimensions and FPS\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    # Create the video writer object\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    #find color range: upper limit first, lower limit second\n",
    "    pixel_color_upper = find_color(imgs[idx])\n",
    "    pixel_color_rgb_upper = cv2.cvtColor(np.array(pixel_color_upper), cv2.COLOR_BGR2RGB)\n",
    "    pixel_color_lower = find_color(imgs[idx])\n",
    "    pixel_color_rgb_lower = cv2.cvtColor(np.array(pixel_color_lower), cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    lower_range = np.array(pixel_color_rgb_lower)\n",
    "    upper_range = np.array(pixel_color_rgb_upper)\n",
    "\n",
    "    while True:\n",
    "        # Read a frame from the video\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "       # Preprocess the frame\n",
    "        #glare filtering\n",
    "        # Convert the frame from RGB to HSV color space\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        # Define the thresholds for the glare color\n",
    "        lower_glare = np.array([0, 0, 200])\n",
    "        upper_glare = np.array([180, 150, 255])\n",
    "        # Create a mask for the glare color\n",
    "        mask = cv2.inRange(hsv, lower_glare, upper_glare)\n",
    "        # Perform morphological operations to remove noise and fill in gaps\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "        mask = cv2.erode(mask, kernel, iterations=2)\n",
    "        mask = cv2.dilate(mask, kernel, iterations=2)\n",
    "        # Apply the matte effect to reduce the intensity of the glare\n",
    "        frame[mask > 0] -= 40\n",
    "\n",
    "        # Perform color detection\n",
    "        mask = cv2.inRange(hsv, lower_range, upper_range)\n",
    "\n",
    "        contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        for contour in contours:\n",
    "            area = cv2.contourArea(contour)\n",
    "            if area > 300:\n",
    "                x, y, w, h = cv2.boundingRect(contour)\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        cv2.imshow('frame', frame)\n",
    "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "\n",
    "        # write the frame to the output video\n",
    "        out.write(frame)\n",
    "\n",
    "        # Display the result\n",
    "        cv2.imshow('result', frame)\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Display the result\n",
    "    out.release()\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778d175d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
